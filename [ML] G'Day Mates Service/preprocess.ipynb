{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"./data/txt2csv_train\"\n",
    "train_merged = \"./data/train_merged\" # ratio of 3 days\n",
    "os.makedirs(train_merged, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"./data/txt2csv_test\"\n",
    "test_processed = \"./data/test\"\n",
    "os.makedirs(test_processed, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert date strings into a standard datetime format\n",
    "def convert_date(date_string):\n",
    "    # Remove dots from the date string\n",
    "    date_string = re.sub(r\"\\\\.\", \"\", date_string)\n",
    "\n",
    "    # Replace Korean AM/PM with English equivalents\n",
    "    date_string = re.sub(r\"오전\", \"AM\", date_string)\n",
    "    date_string = re.sub(r\"오후\", \"PM\", date_string)\n",
    "\n",
    "    # Attempt to match the date string against multiple formats\n",
    "    date_formats = [\n",
    "        \"%Y년 %m월 %d일 %p %I:%M\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y.%m.%d %p %I:%M\",\n",
    "        \"%Y %m %d %p %I:%M\",\n",
    "        \"%Y. %m. %d. %p %I:%M\"\n",
    "    ]\n",
    "\n",
    "    for date_format in date_formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_string, date_format).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    print(f\"Date conversion failed for: {date_string}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: myung_3.csv\n",
      "Processing file: myung_2.csv\n",
      "Processing file: myung_1.csv\n",
      "Processing file: dding_1.csv\n",
      "Processing file: changmin_1.csv\n",
      "Processing file: changmin_2.csv\n",
      "Processing file: changmin_3.csv\n",
      "Processing file: changmin_4.csv\n",
      "Processing file: maeng_3.csv\n",
      "Processing file: maeng_2.csv\n",
      "Processing file: munsik_2.csv\n",
      "Processing file: munsik_3.csv\n",
      "Processing file: munsik_1.csv\n",
      "Processing file: munsik_5.csv\n",
      "Date conversion failed for: direction_and_moment = {\"direction\" : [direction]\n",
      "Date conversion failed for: direction_and_moment = {\"direction\" : [direction]\n",
      "Failed to convert dates in munsik_5.csv\n",
      "     Date         User    Message\n",
      "669  None  \"moment(s)\"  [moment]}\n",
      "682  None  \"moment(s)\"  [moment]}\n",
      "Processing file: munsik_6.csv\n",
      "Saved: ./data/train_merged/train_raw.csv\n",
      "Total files processed: 113\n"
     ]
    }
   ],
   "source": [
    "# List CSV files in the input directory\n",
    "csv_files = [file for file in os.listdir(train) if file.endswith(\".csv\")]\n",
    "total_files_processed = 0\n",
    "\n",
    "merge = []\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Processing file: {csv_file}\")\n",
    "    input_path = os.path.join(train, csv_file)\n",
    "    output_file_prefix = os.path.splitext(csv_file)[0]\n",
    "\n",
    "    # Read the CSV file\n",
    "    dataframe = pd.read_csv(input_path)\n",
    "\n",
    "    # Validate required columns\n",
    "    required_columns = [\"Date\", \"User\", \"Message\"]\n",
    "    if not all(column in dataframe.columns for column in required_columns):\n",
    "        print(f\"Invalid format for {csv_file}: Missing required columns\")\n",
    "        continue\n",
    "\n",
    "    # Convert the date column\n",
    "    dataframe[\"Date\"] = dataframe[\"Date\"].apply(convert_date)\n",
    "\n",
    "    # Handle rows with failed date conversions\n",
    "    failed_dates = dataframe[dataframe[\"Date\"].isna()]\n",
    "    if not failed_dates.empty:\n",
    "        print(f\"Failed to convert dates in {csv_file}\")\n",
    "        print(failed_dates)\n",
    "\n",
    "    # Drop rows with missing or invalid dates\n",
    "    dataframe = dataframe.dropna(subset=[\"Date\"])\n",
    "\n",
    "    # Extract only the date part\n",
    "    dataframe[\"Date\"] = pd.to_datetime(dataframe[\"Date\"]).dt.date\n",
    "\n",
    "    # Filter out short messages (less than 5 characters)\n",
    "    dataframe = dataframe[dataframe[\"Message\"].str.len() >= 5]\n",
    "\n",
    "    # Compute message statistics\n",
    "    message_counts = dataframe.groupby([\"Date\", \"User\"]).size()\n",
    "\n",
    "    # Detect and count images, files, and links\n",
    "    dataframe[\"HasImage\"] = dataframe[\"Message\"].str.contains(\"사진|image\", case=False, na=False)\n",
    "    dataframe[\"HasFile\"] = dataframe[\"Message\"].str.contains(\"파일|file\", case=False, na=False)\n",
    "    dataframe[\"HasLink\"] = dataframe[\"Message\"].str.contains(r\"http\", na=False)\n",
    "\n",
    "    # Combine media-related features\n",
    "    dataframe[\"MediaFlag\"] = dataframe[\"HasImage\"] | dataframe[\"HasFile\"] | dataframe[\"HasLink\"]\n",
    "    media_counts = dataframe.groupby([\"Date\", \"User\"])[\"MediaFlag\"].sum()\n",
    "\n",
    "    # Count questions\n",
    "    dataframe[\"IsQuestion\"] = dataframe[\"Message\"].str.contains(r\"\\\\?\", na=False)\n",
    "    question_counts = dataframe.groupby([\"Date\", \"User\"])[\"IsQuestion\"].sum()\n",
    "\n",
    "    # Calculate average message length\n",
    "    dataframe[\"MessageLength\"] = dataframe[\"Message\"].str.len()\n",
    "    avg_message_length = dataframe.groupby([\"Date\", \"User\"])[\"MessageLength\"].mean()\n",
    "\n",
    "    # Calculate message ratio per user per date\n",
    "    total_daily_messages = dataframe.groupby(\"Date\").size()\n",
    "    message_ratios = (dataframe.groupby([\"Date\", \"User\"]).size() / total_daily_messages).reset_index(name=\"MessageRatio\")\n",
    "\n",
    "    # Create a daily summary dataframe\n",
    "    daily_summary = pd.DataFrame({\n",
    "        \"MessageCount\": message_counts,\n",
    "        \"MessageRatio\": message_ratios.set_index([\"Date\", \"User\"])[\"MessageRatio\"],\n",
    "        \"AvgMessageLength\": avg_message_length\n",
    "    }).fillna(0).reset_index()\n",
    "\n",
    "    # Ensure integer columns are properly cast\n",
    "    daily_summary[\"MessageCount\"] = daily_summary[\"MessageCount\"].astype(int)\n",
    "\n",
    "    # Group data into 3-day intervals\n",
    "    unique_dates = sorted(daily_summary[\"Date\"].unique())\n",
    "    group_count = ceil(len(unique_dates) / 3)\n",
    "    grouped_summaries = []\n",
    "\n",
    "    for group_index in range(group_count):\n",
    "        group_start = group_index * 3\n",
    "        group_end = group_start + 3\n",
    "        group_dates = unique_dates[group_start:group_end]\n",
    "\n",
    "        # Filter data for the current group\n",
    "        group_data = daily_summary[daily_summary[\"Date\"].isin(group_dates)]\n",
    "\n",
    "        # Calculate group-level media statistics\n",
    "        group_media_counts = media_counts.loc[media_counts.index.get_level_values(\"Date\").isin(group_dates)]\n",
    "        total_group_media = group_media_counts.groupby(\"Date\").sum().sum()\n",
    "        user_media_counts = group_media_counts.groupby(\"User\").sum()\n",
    "        user_media_ratios = user_media_counts / total_group_media\n",
    "\n",
    "        # Calculate group-level question statistics\n",
    "        group_question_counts = question_counts.loc[question_counts.index.get_level_values(\"Date\").isin(group_dates)]\n",
    "        total_group_questions = group_question_counts.groupby(\"Date\").sum().sum()\n",
    "        user_question_counts = group_question_counts.groupby(\"User\").sum()\n",
    "        user_question_ratios = user_question_counts / total_group_questions\n",
    "\n",
    "        # Add media and question ratios to the data\n",
    "        group_data = group_data.set_index(\"User\")\n",
    "        group_data[\"MediaRatio\"] = user_media_ratios.fillna(0)\n",
    "        group_data[\"QuestionRatio\"] = user_question_ratios.fillna(0)\n",
    "\n",
    "        # Calculate participation score\n",
    "        participation_score = dataframe.groupby(\"User\")[\"Date\"].nunique() / len(group_dates)\n",
    "        group_data[\"ParticipationScore\"] = group_data.index.map(participation_score.fillna(0))\n",
    "\n",
    "        # Aggregate data\n",
    "        aggregated_data = group_data.groupby(\"User\").agg({\n",
    "            # \"MessageCount\": \"sum\",\n",
    "            \"MessageRatio\": \"mean\",\n",
    "            \"MediaRatio\": \"mean\",\n",
    "            \"QuestionRatio\": \"mean\",\n",
    "            \"AvgMessageLength\": \"mean\",\n",
    "            \"ParticipationScore\": \"mean\"\n",
    "        }).reset_index()\n",
    "\n",
    "        # Add date range to the group data\n",
    "        aggregated_data[\"StartDate\"] = group_dates[0]\n",
    "        aggregated_data[\"EndDate\"] = group_dates[-1] if len(group_dates) > 1 else group_dates[0]\n",
    "        grouped_summaries.append(aggregated_data)\n",
    "        total_files_processed += 1\n",
    "    for aggregated_data in grouped_summaries:\n",
    "        merge.append(aggregated_data)\n",
    "\n",
    "# Save group data to a CSV file\n",
    "group_file_name = f\"train_raw.csv\"\n",
    "group_file_path = os.path.join(train_merged, group_file_name)\n",
    "final_data = pd.concat(merge, ignore_index=True).fillna(0)\n",
    "\n",
    "# print(f\"Final combined data shape: {final_data.shape}\")\n",
    "# duplicate_count = final_data.duplicated().sum()\n",
    "# print(f\"Number of duplicate rows in final data: {duplicate_count}\")\n",
    "\n",
    "\n",
    "final_data.to_csv(group_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Saved: {group_file_path}\")\n",
    "print(f\"Total files processed: {total_files_processed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List CSV files in the input directory\n",
    "csv_files = [file for file in os.listdir(test) if file.endswith(\".csv\")]\n",
    "total_files_processed = 0\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Processing file: {csv_file}\")\n",
    "    input_path = os.path.join(test, csv_file)\n",
    "    output_file_prefix = os.path.splitext(csv_file)[0]\n",
    "\n",
    "    # Read the CSV file\n",
    "    dataframe = pd.read_csv(input_path)\n",
    "\n",
    "    # Validate required columns\n",
    "    required_columns = [\"Date\", \"User\", \"Message\"]\n",
    "    if not all(column in dataframe.columns for column in required_columns):\n",
    "        print(f\"Invalid format for {csv_file}: Missing required columns\")\n",
    "        continue\n",
    "\n",
    "    # Convert the date column\n",
    "    dataframe[\"Date\"] = dataframe[\"Date\"].apply(convert_date)\n",
    "\n",
    "    # Handle rows with failed date conversions\n",
    "    failed_dates = dataframe[dataframe[\"Date\"].isna()]\n",
    "    if not failed_dates.empty:\n",
    "        print(f\"Failed to convert dates in {csv_file}\")\n",
    "        print(failed_dates)\n",
    "\n",
    "    # Drop rows with missing or invalid dates\n",
    "    dataframe = dataframe.dropna(subset=[\"Date\"])\n",
    "\n",
    "    # Extract only the date part\n",
    "    dataframe[\"Date\"] = pd.to_datetime(dataframe[\"Date\"]).dt.date\n",
    "\n",
    "    # Filter out short messages (less than 5 characters)\n",
    "    dataframe = dataframe[dataframe[\"Message\"].str.len() >= 5]\n",
    "\n",
    "    # Detect and count images, files, and links\n",
    "    dataframe[\"HasImage\"] = dataframe[\"Message\"].str.contains(\"사진|image\", case=False, na=False)\n",
    "    dataframe[\"HasFile\"] = dataframe[\"Message\"].str.contains(\"파일|file\", case=False, na=False)\n",
    "    dataframe[\"HasLink\"] = dataframe[\"Message\"].str.contains(r\"http\", na=False)\n",
    "\n",
    "    # Combine media-related features\n",
    "    dataframe[\"MediaFlag\"] = dataframe[\"HasImage\"] | dataframe[\"HasFile\"] | dataframe[\"HasLink\"]\n",
    "\n",
    "    # Count questions\n",
    "    dataframe[\"IsQuestion\"] = dataframe[\"Message\"].str.contains(r\"\\\\?\", na=False)\n",
    "\n",
    "    # Calculate average message length\n",
    "    dataframe[\"MessageLength\"] = dataframe[\"Message\"].str.len()\n",
    "\n",
    "    # Group data into 3-day intervals\n",
    "    unique_dates = sorted(dataframe[\"Date\"].unique())\n",
    "    group_count = ceil(len(unique_dates) / 3)\n",
    "    grouped_summaries = []\n",
    "\n",
    "    for group_index in range(group_count):\n",
    "        group_start = group_index * 3\n",
    "        group_end = group_start + 3\n",
    "        group_dates = unique_dates[group_start:group_end]\n",
    "\n",
    "        # Filter data for the current group\n",
    "        group_data = dataframe[dataframe[\"Date\"].isin(group_dates)]\n",
    "\n",
    "        # Calculate message ratios\n",
    "        total_messages = group_data.groupby(\"Date\").size().sum()  # Total messages in 3 days\n",
    "        user_message_counts = group_data.groupby(\"User\").size()  # User-level message counts\n",
    "        user_message_ratios = user_message_counts / total_messages.fillna(0)  # User-level ratios\n",
    "\n",
    "        # Calculate media ratios\n",
    "        total_media = group_data[\"MediaFlag\"].sum()\n",
    "        user_media_counts = group_data.groupby(\"User\")[\"MediaFlag\"].sum()\n",
    "        user_media_ratios = user_media_counts / total_media.fillna(0)\n",
    "\n",
    "        # Calculate question ratios\n",
    "        total_questions = group_data[\"IsQuestion\"].sum()\n",
    "        user_question_counts = group_data.groupby(\"User\")[\"IsQuestion\"].sum()\n",
    "        user_question_ratios = user_question_counts / total_questions.fillna(0)\n",
    "\n",
    "        # Aggregate data\n",
    "        aggregated_data = pd.DataFrame({\n",
    "            \"MessageRatio\": user_message_ratios,\n",
    "            \"MediaRatio\": user_media_ratios,\n",
    "            \"QuestionRatio\": user_question_ratios,\n",
    "            \"AvgMessageLength\": group_data.groupby(\"User\")[\"MessageLength\"].mean(),\n",
    "            \"ParticipationScore\": group_data.groupby(\"User\")[\"Date\"].nunique() / len(group_dates)\n",
    "        }).fillna(0).reset_index()\n",
    "\n",
    "        # Add date range to the group data\n",
    "        # aggregated_data[\"StartDate\"] = group_dates[0]\n",
    "        # aggregated_data[\"EndDate\"] = group_dates[-1] if len(group_dates) > 1 else group_dates[0]\n",
    "\n",
    "        grouped_summaries.append(aggregated_data)\n",
    "\n",
    "        # Save group data to a CSV file\n",
    "        group_file_name = f\"{output_file_prefix}_3day_group{group_index + 1}.csv\"\n",
    "        group_file_path = os.path.join(test_processed, group_file_name)\n",
    "        aggregated_data.to_csv(group_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Saved: {group_file_path}\")\n",
    "\n",
    "        total_files_processed += 1\n",
    "\n",
    "print(f\"Total files processed: {total_files_processed}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MessageRatio</th>\n",
       "      <th>MediaRatio</th>\n",
       "      <th>QuestionRatio</th>\n",
       "      <th>AvgMessageLength</th>\n",
       "      <th>ParticipationScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.256603</td>\n",
       "      <td>0.169399</td>\n",
       "      <td>0.205829</td>\n",
       "      <td>35.963433</td>\n",
       "      <td>6.584699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.173435</td>\n",
       "      <td>0.266786</td>\n",
       "      <td>0.149741</td>\n",
       "      <td>92.770311</td>\n",
       "      <td>3.699746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.015278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.121032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160142</td>\n",
       "      <td>22.875000</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.355777</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>33.246970</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1373.166667</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MessageRatio  MediaRatio  QuestionRatio  AvgMessageLength  \\\n",
       "count    549.000000  549.000000     549.000000        549.000000   \n",
       "mean       0.256603    0.169399       0.205829         35.963433   \n",
       "std        0.173435    0.266786       0.149741         92.770311   \n",
       "min        0.015278    0.000000       0.013514          5.000000   \n",
       "25%        0.121032    0.000000       0.083333         15.500000   \n",
       "50%        0.213333    0.000000       0.160142         22.875000   \n",
       "75%        0.355777    0.250000       0.303030         33.246970   \n",
       "max        1.000000    1.000000       0.800000       1373.166667   \n",
       "\n",
       "       ParticipationScore  \n",
       "count          549.000000  \n",
       "mean             6.584699  \n",
       "std              3.699746  \n",
       "min              0.666667  \n",
       "25%              4.000000  \n",
       "50%              5.666667  \n",
       "75%              8.000000  \n",
       "max             38.000000  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549 entries, 0 to 548\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   User                549 non-null    object \n",
      " 1   MessageRatio        549 non-null    float64\n",
      " 2   MediaRatio          549 non-null    float64\n",
      " 3   QuestionRatio       549 non-null    float64\n",
      " 4   AvgMessageLength    549 non-null    float64\n",
      " 5   ParticipationScore  549 non-null    float64\n",
      " 6   StartDate           549 non-null    object \n",
      " 7   EndDate             549 non-null    object \n",
      "dtypes: float64(5), object(3)\n",
      "memory usage: 34.4+ KB\n"
     ]
    }
   ],
   "source": [
    "final_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     549.000000\n",
      "mean       35.963433\n",
      "std        92.770311\n",
      "min         5.000000\n",
      "25%        15.500000\n",
      "50%        22.875000\n",
      "75%        33.246970\n",
      "max      1373.166667\n",
      "Name: AvgMessageLength, dtype: float64\n",
      "Outlier boundaries: Lower = -11.12045454545455, Upper = 59.86742424242425\n",
      "Number of outliers: 40\n",
      "Shape after removing outliers: (509, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Describe statistics for AvgMessageLength\n",
    "avg_message_stats = final_data[\"AvgMessageLength\"].describe()\n",
    "print(avg_message_stats)\n",
    "\n",
    "# Calculate interquartile range (IQR)\n",
    "Q1 = avg_message_stats[\"25%\"]\n",
    "Q3 = avg_message_stats[\"75%\"]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Outlier boundaries: Lower = {lower_bound}, Upper = {upper_bound}\")\n",
    "\n",
    "# Identify outliers\n",
    "outliers = final_data[(final_data[\"AvgMessageLength\"] < lower_bound) | (final_data[\"AvgMessageLength\"] > upper_bound)]\n",
    "print(f\"Number of outliers: {outliers.shape[0]}\")\n",
    "\n",
    "# Remove outliers\n",
    "filtered_data = final_data[(final_data[\"AvgMessageLength\"] >= lower_bound) & (final_data[\"AvgMessageLength\"] <= upper_bound)]\n",
    "print(f\"Shape after removing outliers: {filtered_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MessageRatio</th>\n",
       "      <th>MediaRatio</th>\n",
       "      <th>QuestionRatio</th>\n",
       "      <th>AvgMessageLength</th>\n",
       "      <th>ParticipationScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>509.000000</td>\n",
       "      <td>509.000000</td>\n",
       "      <td>509.000000</td>\n",
       "      <td>509.000000</td>\n",
       "      <td>509.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.254073</td>\n",
       "      <td>0.170851</td>\n",
       "      <td>0.205079</td>\n",
       "      <td>23.406219</td>\n",
       "      <td>6.623445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.172803</td>\n",
       "      <td>0.266853</td>\n",
       "      <td>0.148564</td>\n",
       "      <td>10.969778</td>\n",
       "      <td>3.768783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.015278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.117667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.210609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>21.733333</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.354224</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>57.416667</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MessageRatio  MediaRatio  QuestionRatio  AvgMessageLength  \\\n",
       "count    509.000000  509.000000     509.000000        509.000000   \n",
       "mean       0.254073    0.170851       0.205079         23.406219   \n",
       "std        0.172803    0.266853       0.148564         10.969778   \n",
       "min        0.015278    0.000000       0.013514          5.000000   \n",
       "25%        0.117667    0.000000       0.083333         15.000000   \n",
       "50%        0.210609    0.000000       0.163636         21.733333   \n",
       "75%        0.354224    0.250000       0.303030         30.000000   \n",
       "max        1.000000    1.000000       0.800000         57.416667   \n",
       "\n",
       "       ParticipationScore  \n",
       "count          509.000000  \n",
       "mean             6.623445  \n",
       "std              3.768783  \n",
       "min              1.666667  \n",
       "25%              4.000000  \n",
       "50%              5.666667  \n",
       "75%              8.000000  \n",
       "max             38.000000  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to: ./data/train_merged/train.csv\n"
     ]
    }
   ],
   "source": [
    "output_file_path = os.path.join(train_merged, \"train.csv\")\n",
    "filtered_data.to_csv(output_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Filtered data saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: munsik_4.csv\n",
      "Saved: ./data/test/munsik_4_3day_group1.csv\n",
      "Saved: ./data/test/munsik_4_3day_group2.csv\n",
      "Saved: ./data/test/munsik_4_3day_group3.csv\n",
      "Saved: ./data/test/munsik_4_3day_group4.csv\n",
      "Total files processed: 4\n"
     ]
    }
   ],
   "source": [
    "# List CSV files in the input directory\n",
    "csv_files = [file for file in os.listdir(test) if file.endswith(\".csv\")]\n",
    "total_files_processed = 0\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Processing file: {csv_file}\")\n",
    "    input_path = os.path.join(test, csv_file)\n",
    "    output_file_prefix = os.path.splitext(csv_file)[0]\n",
    "\n",
    "    # Read the CSV file\n",
    "    dataframe = pd.read_csv(input_path)\n",
    "\n",
    "    # Validate required columns\n",
    "    required_columns = [\"Date\", \"User\", \"Message\"]\n",
    "    if not all(column in dataframe.columns for column in required_columns):\n",
    "        print(f\"Invalid format for {csv_file}: Missing required columns\")\n",
    "        continue\n",
    "\n",
    "    # Convert the date column\n",
    "    dataframe[\"Date\"] = dataframe[\"Date\"].apply(convert_date)\n",
    "\n",
    "    # Handle rows with failed date conversions\n",
    "    failed_dates = dataframe[dataframe[\"Date\"].isna()]\n",
    "    if not failed_dates.empty:\n",
    "        print(f\"Failed to convert dates in {csv_file}\")\n",
    "        print(failed_dates)\n",
    "\n",
    "    # Drop rows with missing or invalid dates\n",
    "    dataframe = dataframe.dropna(subset=[\"Date\"])\n",
    "\n",
    "    # Extract only the date part\n",
    "    dataframe[\"Date\"] = pd.to_datetime(dataframe[\"Date\"]).dt.date\n",
    "\n",
    "    # Filter out short messages (less than 5 characters)\n",
    "    dataframe = dataframe[dataframe[\"Message\"].str.len() >= 5]\n",
    "\n",
    "    # Compute message statistics\n",
    "    message_counts = dataframe.groupby([\"Date\", \"User\"]).size()\n",
    "\n",
    "    # Detect and count images, files, and links\n",
    "    dataframe[\"HasImage\"] = dataframe[\"Message\"].str.contains(\"사진|image\", case=False, na=False)\n",
    "    dataframe[\"HasFile\"] = dataframe[\"Message\"].str.contains(\"파일|file\", case=False, na=False)\n",
    "    dataframe[\"HasLink\"] = dataframe[\"Message\"].str.contains(r\"http\", na=False)\n",
    "\n",
    "    # Combine media-related features\n",
    "    dataframe[\"MediaFlag\"] = dataframe[\"HasImage\"] | dataframe[\"HasFile\"] | dataframe[\"HasLink\"]\n",
    "    media_counts = dataframe.groupby([\"Date\", \"User\"])[\"MediaFlag\"].sum()\n",
    "\n",
    "    # Count questions\n",
    "    dataframe[\"IsQuestion\"] = dataframe[\"Message\"].str.contains(r\"\\\\?\", na=False)\n",
    "    question_counts = dataframe.groupby([\"Date\", \"User\"])[\"IsQuestion\"].sum()\n",
    "\n",
    "    # Calculate average message length\n",
    "    dataframe[\"MessageLength\"] = dataframe[\"Message\"].str.len()\n",
    "    avg_message_length = dataframe.groupby([\"Date\", \"User\"])[\"MessageLength\"].mean()\n",
    "\n",
    "    # Calculate message ratio per user per date\n",
    "    total_daily_messages = dataframe.groupby(\"Date\").size()\n",
    "    message_ratios = (dataframe.groupby([\"Date\", \"User\"]).size() / total_daily_messages).reset_index(name=\"MessageRatio\")\n",
    "\n",
    "    # Create a daily summary dataframe\n",
    "    daily_summary = pd.DataFrame({\n",
    "        \"MessageCount\": message_counts,\n",
    "        \"MessageRatio\": message_ratios.set_index([\"Date\", \"User\"])[\"MessageRatio\"],\n",
    "        \"AvgMessageLength\": avg_message_length\n",
    "    }).fillna(0).reset_index()\n",
    "\n",
    "    # Ensure integer columns are properly cast\n",
    "    daily_summary[\"MessageCount\"] = daily_summary[\"MessageCount\"].astype(int)\n",
    "\n",
    "    # Group data into 3-day intervals\n",
    "    unique_dates = sorted(daily_summary[\"Date\"].unique())\n",
    "    group_count = ceil(len(unique_dates) / 3)\n",
    "    grouped_summaries = []\n",
    "\n",
    "    for group_index in range(group_count):\n",
    "        group_start = group_index * 3\n",
    "        group_end = group_start + 3\n",
    "        group_dates = unique_dates[group_start:group_end]\n",
    "\n",
    "        # Filter data for the current group\n",
    "        group_data = daily_summary[daily_summary[\"Date\"].isin(group_dates)]\n",
    "\n",
    "        # Calculate group-level media statistics\n",
    "        group_media_counts = media_counts.loc[media_counts.index.get_level_values(\"Date\").isin(group_dates)]\n",
    "        total_group_media = group_media_counts.groupby(\"Date\").sum().sum()\n",
    "        user_media_counts = group_media_counts.groupby(\"User\").sum()\n",
    "        user_media_ratios = user_media_counts / total_group_media\n",
    "\n",
    "        # Calculate group-level question statistics\n",
    "        group_question_counts = question_counts.loc[question_counts.index.get_level_values(\"Date\").isin(group_dates)]\n",
    "        total_group_questions = group_question_counts.groupby(\"Date\").sum().sum()\n",
    "        user_question_counts = group_question_counts.groupby(\"User\").sum()\n",
    "        user_question_ratios = user_question_counts / total_group_questions\n",
    "\n",
    "        # Add media and question ratios to the data\n",
    "        group_data = group_data.set_index(\"User\")\n",
    "        group_data[\"MediaRatio\"] = user_media_ratios.fillna(0)\n",
    "        group_data[\"QuestionRatio\"] = user_question_ratios.fillna(0)\n",
    "\n",
    "        # Calculate participation score\n",
    "        participation_score = dataframe.groupby(\"User\")[\"Date\"].nunique() / len(group_dates)\n",
    "        group_data[\"ParticipationScore\"] = group_data.index.map(participation_score.fillna(0))\n",
    "\n",
    "        # Aggregate data\n",
    "        aggregated_data = group_data.groupby(\"User\").agg({\n",
    "            # \"MessageCount\": \"sum\",\n",
    "            \"MessageRatio\": \"mean\",\n",
    "            \"MediaRatio\": \"mean\",\n",
    "            \"QuestionRatio\": \"mean\",\n",
    "            \"AvgMessageLength\": \"mean\",\n",
    "            \"ParticipationScore\": \"mean\"\n",
    "        }).reset_index()\n",
    "\n",
    "        # Add date range to the group data\n",
    "        # aggregated_data[\"StartDate\"] = group_dates[0]\n",
    "        # aggregated_data[\"EndDate\"] = group_dates[-1] if len(group_dates) > 1 else group_dates[0]\n",
    "\n",
    "        grouped_summaries.append(aggregated_data)\n",
    "\n",
    "        # Save group data to a CSV file\n",
    "        group_file_name = f\"{output_file_prefix}_3day_group{group_index + 1}.csv\"\n",
    "        group_file_path = os.path.join(test_processed, group_file_name)\n",
    "        aggregated_data.to_csv(group_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Saved: {group_file_path}\")\n",
    "\n",
    "        total_files_processed += 1\n",
    "\n",
    "print(f\"Total files processed: {total_files_processed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List CSV files in the input directory\n",
    "csv_files = [file for file in os.listdir(test) if file.endswith(\".csv\")]\n",
    "total_files_processed = 0\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Processing file: {csv_file}\")\n",
    "    input_path = os.path.join(test, csv_file)\n",
    "    output_file_prefix = os.path.splitext(csv_file)[0]\n",
    "\n",
    "    # Read the CSV file\n",
    "    dataframe = pd.read_csv(input_path)\n",
    "\n",
    "    # Validate required columns\n",
    "    required_columns = [\"Date\", \"User\", \"Message\"]\n",
    "    if not all(column in dataframe.columns for column in required_columns):\n",
    "        print(f\"Invalid format for {csv_file}: Missing required columns\")\n",
    "        continue\n",
    "\n",
    "    # Convert the date column\n",
    "    dataframe[\"Date\"] = dataframe[\"Date\"].apply(convert_date)\n",
    "\n",
    "    # Handle rows with failed date conversions\n",
    "    failed_dates = dataframe[dataframe[\"Date\"].isna()]\n",
    "    if not failed_dates.empty:\n",
    "        print(f\"Failed to convert dates in {csv_file}\")\n",
    "        print(failed_dates)\n",
    "\n",
    "    # Drop rows with missing or invalid dates\n",
    "    dataframe = dataframe.dropna(subset=[\"Date\"])\n",
    "\n",
    "    # Extract only the date part\n",
    "    dataframe[\"Date\"] = pd.to_datetime(dataframe[\"Date\"]).dt.date\n",
    "\n",
    "    # Filter out short messages (less than 5 characters)\n",
    "    dataframe = dataframe[dataframe[\"Message\"].str.len() >= 5]\n",
    "\n",
    "    # Compute message statistics\n",
    "    message_counts = dataframe.groupby([\"Date\", \"User\"]).size()\n",
    "\n",
    "    # Detect and count images, files, and links\n",
    "    dataframe[\"HasImage\"] = dataframe[\"Message\"].str.contains(\"사진|image\", case=False, na=False)\n",
    "    dataframe[\"HasFile\"] = dataframe[\"Message\"].str.contains(\"파일|file\", case=False, na=False)\n",
    "    dataframe[\"HasLink\"] = dataframe[\"Message\"].str.contains(r\"http\", na=False)\n",
    "\n",
    "    # Combine media-related features\n",
    "    dataframe[\"MediaFlag\"] = dataframe[\"HasImage\"] | dataframe[\"HasFile\"] | dataframe[\"HasLink\"]\n",
    "    media_counts = dataframe.groupby([\"Date\", \"User\"])[\"MediaFlag\"].sum()\n",
    "\n",
    "    # Count questions\n",
    "    dataframe[\"IsQuestion\"] = dataframe[\"Message\"].str.contains(r\"\\\\?\", na=False)\n",
    "    question_counts = dataframe.groupby([\"Date\", \"User\"])[\"IsQuestion\"].sum()\n",
    "\n",
    "    # Calculate average message length\n",
    "    dataframe[\"MessageLength\"] = dataframe[\"Message\"].str.len()\n",
    "    avg_message_length = dataframe.groupby([\"Date\", \"User\"])[\"MessageLength\"].mean()\n",
    "\n",
    "    # Calculate message ratio per user per date\n",
    "    total_daily_messages = dataframe.groupby(\"Date\").size()\n",
    "    message_ratios = (dataframe.groupby([\"Date\", \"User\"]).size() / total_daily_messages).reset_index(name=\"MessageRatio\")\n",
    "\n",
    "    # Create a daily summary dataframe\n",
    "    daily_summary = pd.DataFrame({\n",
    "        \"MessageCount\": message_counts,\n",
    "        \"MessageRatio\": message_ratios.set_index([\"Date\", \"User\"])[\"MessageRatio\"],\n",
    "        \"AvgMessageLength\": avg_message_length\n",
    "    }).fillna(0).reset_index()\n",
    "\n",
    "    # Ensure integer columns are properly cast\n",
    "    daily_summary[\"MessageCount\"] = daily_summary[\"MessageCount\"].astype(int)\n",
    "\n",
    "    # Group data into 3-day intervals\n",
    "    unique_dates = sorted(daily_summary[\"Date\"].unique())\n",
    "    group_count = ceil(len(unique_dates) / 3)\n",
    "    grouped_summaries = []\n",
    "\n",
    "    for group_index in range(group_count):\n",
    "        group_start = group_index * 3\n",
    "        group_end = group_start + 3\n",
    "        group_dates = unique_dates[group_start:group_end]\n",
    "\n",
    "        # Filter data for the current group\n",
    "        group_data = daily_summary[daily_summary[\"Date\"].isin(group_dates)]\n",
    "         \n",
    "        # Calculate group-level message statistics\n",
    "        group_message_counts = message_counts.loc[message_counts.index.get_level_values(\"Date\").isin(group_dates)]\n",
    "        total_group_message = group_message_counts.groupby(\"Date\").sum().sum()\n",
    "        user_message_counts = group_message_counts.groupby(\"User\").sum()\n",
    "        user_message_ratios = user_message_counts / total_group_message\n",
    "        \n",
    "        # Calculate group-level media statistics\n",
    "        group_media_counts = media_counts.loc[media_counts.index.get_level_values(\"Date\").isin(group_dates)]\n",
    "        total_group_media = group_media_counts.groupby(\"Date\").sum().sum()\n",
    "        user_media_counts = group_media_counts.groupby(\"User\").sum()\n",
    "        user_media_ratios = user_media_counts / total_group_media\n",
    "\n",
    "        # Calculate group-level question statistics\n",
    "        group_question_counts = question_counts.loc[question_counts.index.get_level_values(\"Date\").isin(group_dates)]\n",
    "        total_group_questions = group_question_counts.groupby(\"Date\").sum().sum()\n",
    "        user_question_counts = group_question_counts.groupby(\"User\").sum()\n",
    "        user_question_ratios = user_question_counts / total_group_questions\n",
    "\n",
    "        # Add media and question ratios to the data\n",
    "        group_data = group_data.set_index(\"User\")\n",
    "        group_data[\"MessageRatio\"] = user_message_ratios.fillna(0)\n",
    "        group_data[\"MediaRatio\"] = user_media_ratios.fillna(0)\n",
    "        group_data[\"QuestionRatio\"] = user_question_ratios.fillna(0)\n",
    "\n",
    "        # Calculate participation score\n",
    "        participation_score = dataframe.groupby(\"User\")[\"Date\"].nunique() / len(group_dates)\n",
    "        group_data[\"ParticipationScore\"] = group_data.index.map(participation_score.fillna(0))\n",
    "\n",
    "        # Aggregate data\n",
    "        aggregated_data = group_data.groupby(\"User\").agg({\n",
    "            # \"MessageCount\": \"sum\",\n",
    "            \"MessageRatio\": \"mean\",\n",
    "            \"MediaRatio\": \"mean\",\n",
    "            \"QuestionRatio\": \"mean\",\n",
    "            \"AvgMessageLength\": \"mean\",\n",
    "            \"ParticipationScore\": \"mean\"\n",
    "        }).reset_index()\n",
    "\n",
    "        # Add date range to the group data\n",
    "        # aggregated_data[\"StartDate\"] = group_dates[0]\n",
    "        # aggregated_data[\"EndDate\"] = group_dates[-1] if len(group_dates) > 1 else group_dates[0]\n",
    "\n",
    "        grouped_summaries.append(aggregated_data)\n",
    "\n",
    "        # Save group data to a CSV file\n",
    "        group_file_name = f\"{output_file_prefix}_3day_group{group_index + 1}.csv\"\n",
    "        group_file_path = os.path.join(test_processed, group_file_name)\n",
    "        aggregated_data.to_csv(group_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Saved: {group_file_path}\")\n",
    "\n",
    "        total_files_processed += 1\n",
    "\n",
    "print(f\"Total files processed: {total_files_processed}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
